모델이 생성한 가사: '<start> he s a monster <end> '

- 토큰화 과정에서 최대길이를 15로 해주지않아 학습에 시간이 매우오래걸려 문제점을 찾는것이 힘들었다.
최대길이를 지정해주지 않으니 토큰사이에 0인 pad들이 무수히 많이껴있어서 학습이 더디지 않았나 싶다.
- 단어들끼리의 연관성이있어야 모델이 다음에 올 단어를 예측하기가 쉬울것이다. 
- 학습 길이를 길게해주면 가사뿐만아니라 일기나 소설을 만드는 인공지능에도 적용해볼수있을것 같다는 생각이 들었다.
